{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-10 15:02:55.090223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-10 15:02:55.090317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-10 15:02:55.092546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 15:02:55.099013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 15:02:55.837524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "\n",
    "import torch_influence\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_warn_always(False)\n",
    "\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import yaml\n",
    "import lm_eval\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training ,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from llm import get_tokenizer_and_model\n",
    "\n",
    "tokenizer, model = get_tokenizer_and_model(model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = model.to(\"cuda:5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 11679/11679 [00:00<00:00, 258220.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 1000/1000 [00:00<00:00, 233496.85 examples/s]\n",
      "Generating test split: 100%|██████████| 1000/1000 [00:00<00:00, 229850.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"allenai/sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = dataset[\"test\"][100:120]['question'] # take 10 samples for ground truth for now\n",
    "train_gen = dataset[\"train\"][100:120]['question'] # take 10 samples for ground truth for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10:15:03:20,827 INFO     [SentenceTransformer.py:218] Load pretrained SentenceTransformer: Alibaba-NLP/gte-Qwen2-1.5B-instruct\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.59it/s]\n",
      "2025-02-10:15:03:25,769 INFO     [SentenceTransformer.py:357] 1 prompts are loaded, with the keys: ['query']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True, device=\"cpu\")\n",
    "embedding_model = embedding_model.to(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, AnyStr\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "temp = 0.99\n",
    "\n",
    "# embed a list of texts\n",
    "def embed(data : List[AnyStr]) -> torch.Tensor:\n",
    "    max_length = 32768\n",
    "    passage_embeddings = embedding_model.encode(data)\n",
    "    # normalize embeddings\n",
    "    query_embeddings = normalize(passage_embeddings)\n",
    "    return query_embeddings\n",
    "\n",
    "# mmd function\n",
    "def rbf_mmd(X, Y, sigma=1.0, chunk_size=None):\n",
    "    gamma = 1 / (2 * sigma**2)\n",
    "    def row_mean(v, X):\n",
    "        dist_sqrs = torch.sum((X - v)**2, dim=1)\n",
    "        return torch.exp(-gamma * dist_sqrs).mean()\n",
    "    kernel_X = lambda v: row_mean(v, X)\n",
    "    kernel_Y = lambda v: row_mean(v, Y)\n",
    "    K_XX = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(X))\n",
    "    K_XY = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(Y))\n",
    "    K_YY = torch.mean(torch.vmap(kernel_Y, chunk_size=chunk_size)(Y))\n",
    "    return K_XX + K_YY - 2 * K_XY\n",
    "\n",
    "# generate a single sample from LLM, based on 3 examples. \n",
    "# can set temperature higher to get more diverse responses.\n",
    "def generate_response(model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are my assistant. Please look at the examples of questions given and write a similar question with the same topic or flavour. Do not give the solution or any extra words.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\\n\".join(dataset[\"train\"][0:5]['question'])},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "    return model.get_input_embeddings()(input_ids), input_ids, output\n",
    "\n",
    "# given a extract_string text response, find the logits from an LLM and backpropagate the gradients to the embedding values\n",
    "# input_ids are the input prompts used (so we start generating log-probs from that point)\n",
    "def backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, model_output, input_ids):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False # freeze all params\n",
    "\n",
    "    output_ids = tokenizer(model_output, return_tensors=\"pt\").input_ids.to(\"cuda:5\")\n",
    "    # Concatenate input and output to form full sequence. This makes it easy to find the logit later.\n",
    "    full_input = torch.cat([input_ids, output_ids], dim=-1)\n",
    "\n",
    "    # **Extract embeddings with requires_grad=True**\n",
    "    embedding_layer = model.get_input_embeddings()  # Embedding layer\n",
    "    input_embeds = embedding_layer(full_input).detach().clone()  # Shape: (1, input_length + response length, hidden_size)\n",
    "\n",
    "    # allow gradients on input embedding\n",
    "    input_embeds.requires_grad = True  # Enable gradient tracking\n",
    "    optimizer = torch.optim.Adam([input_embeds], lr=1.0)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs_embeds=input_embeds, disable_tqdm=True)\n",
    "\n",
    "    # Extract logits (this is the full sentence logit)\n",
    "    logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "    # Compute log-softmax to get log probabilities\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Shift output_ids for teacher forcing (we predict the next token)\n",
    "    target_ids = full_input[:, 1:]  # Shift left for alignment\n",
    "\n",
    "    # Gather log probabilities corresponding to the actual output tokens\n",
    "    output_log_probs = log_probs[:, :-1, :].gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Compute total negative log-likelihood starting only from end of input (so only log-probs of response)\n",
    "    total_log_likelihood = - output_log_probs[:, input_ids.shape[-1]:].sum()\n",
    "\n",
    "    # Backpropagate\n",
    "    total_log_likelihood.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return input_embeds # return the input embedding after it has been updated with gradients\n",
    "\n",
    "# generate an LLM response with an input embedding\n",
    "def generate_response_with_embedding(max_new_tokens, model, new_embed, num_samples, input_ids):\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        generated_tokens = input_ids.clone()\n",
    "        new_generation_input_embed = new_embed.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(inputs_embeds=new_generation_input_embed, disable_tqdm=True)\n",
    "            logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
    "            \n",
    "            #next_token = torch.argmax(logits, dim=-1, keepdim=True)  # Greedy decoding\n",
    "            \n",
    "            # temperature\n",
    "            logits = logits / temp\n",
    "            # Convert to probabilities and sample\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append new token to generated sequence\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "            # Update `input_embeds` to include new token embeddings\n",
    "            next_token_embedding = model.get_input_embeddings()(next_token)\n",
    "            new_generation_input_embed = torch.cat([new_generation_input_embed, next_token_embedding], dim=1)\n",
    "\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        samples.append(tokenizer.decode(generated_tokens[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run REINFORCE to update embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting embedding to start gradient descent\n",
    "prompt_input_embeds, input_ids, output = generate_response(model, tokenizer)\n",
    "\n",
    "n=10 # n, same as formula\n",
    "k=20 # k, same as formula\n",
    "lr=0.0001 # learning rate\n",
    "training_steps = 3 # number of GD steps.\n",
    "for step in range(training_steps):\n",
    "    print(\"training step: \", step)\n",
    "    all_estimated_gradients = []\n",
    "    all_similarity = []\n",
    "    for idx in range(n):\n",
    "        print(\"getting gradient samples in iteration: \", idx)\n",
    "        sampled_examples = [] # \n",
    "        backpropagated_input_embeddings = []\n",
    "        for _ in range(k):\n",
    "            input_len = len(input_ids[0])\n",
    "            \n",
    "            # generate a random LLM response from current embedding\n",
    "            output = generate_response_with_embedding(256, model, prompt_input_embeds, 1, input_ids)[0]\n",
    "            \n",
    "            # backpropagate to update the embedding\n",
    "            updated_input_embeds = backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, output, input_ids)\n",
    "            sampled_examples.append(output)\n",
    "            backpropagated_input_embeddings.append(updated_input_embeds)\n",
    "\n",
    "        logit_grad = torch.zeros_like(backpropagated_input_embeddings[0][:,:input_len,:]) # log p(x_1) + log p(x_2) + ... + p(x_k)\n",
    "        for embedding in backpropagated_input_embeddings:\n",
    "            \n",
    "            # calculate the gradient based on updated embedding i.e., derivative of log P(X)\n",
    "            grad_log_prob = embedding[:,:input_len,:] - prompt_input_embeds[:,:input_len,:]\n",
    "            \n",
    "            # log P(X_1) + ... + P(X_k)\n",
    "            logit_grad += grad_log_prob\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # compute MMD of current k samples\n",
    "            mmd = rbf_mmd(torch.tensor(embed(sampled_examples)), torch.tensor(embed(ground_truth))) # MMD value\n",
    "\n",
    "        all_similarity.append(mmd)\n",
    "        estimated_gradient_sample = mmd * logit_grad # REINFORCE equation to get one sample of logit gradient\n",
    "        all_estimated_gradients.append(estimated_gradient_sample)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(\"embedding norm before gradient update: \", prompt_input_embeds[:,:input_len,:].sum())\n",
    "        print(\"average MMD values before updated: \", np.array(all_similarity).mean())\n",
    "        expected_gradient = torch.stack(all_estimated_gradients).sum(dim=0) * (1/n) # expected gradient\n",
    "        prompt_input_embeds = prompt_input_embeds[:,:input_len,:] - lr * expected_gradient # update embedding with gradient\n",
    "        print(\"embedding norm after gradient update: \", prompt_input_embeds[:,:input_len,:].sum())\n",
    "\n",
    "        # check MMD for newly generated samples\n",
    "        max_new_tokens=256\n",
    "        num_samples = 10\n",
    "        generated_tokens = input_ids.clone()\n",
    "        new_samples = generate_response_with_embedding(max_new_tokens, model, prompt_input_embeds, num_samples, input_ids)\n",
    "        print(\"new MMD values after gradient update: \", rbf_mmd(torch.tensor(embed(new_samples)), torch.tensor(embed(ground_truth))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where is the spinal trigeminal nucleus located?',\n",
       " 'The lithosphere is divided into a dozen major and several minor what?',\n",
       " 'During the first year after birth, what is a baby called?',\n",
       " 'What are used to indicate the number of atoms of an element that are in the compound?',\n",
       " 'Area, volume, and speed are all examples of what type of units?',\n",
       " 'Anything moving has what type of energy?',\n",
       " 'A skydiver will reach what when the air drag equals their weight?',\n",
       " 'What organs are considered the female gonads?',\n",
       " 'What is the adaptation that certain animals use to become less visible to predators and prey?',\n",
       " 'What is another term for blood clotting?',\n",
       " 'What do you call the study of how organisms interact with their environment and each other?',\n",
       " 'Childbirth usually starts when which sac breaks?',\n",
       " 'What phenomenon, which is most important in small populations, occurs because the alleles in an offspring generation are a random sample of the alleles in the parent generation?',\n",
       " 'What is the term for when an embryo fixes itself to the side of the uterus?',\n",
       " 'When exposed to ultraviolet, some substances, such as minerals, glow in characteristic visible wavelengths, a process called this?',\n",
       " 'What disease occurs when the cell cycle is no longer regulated?',\n",
       " 'If a lump of clay is dropped into water, what will occur?',\n",
       " 'Nerve cells that sense touch are found mainly where?',\n",
       " 'What do concentric circles on a topographic map indicate?',\n",
       " 'Arthropods today have two unusual hox genes, both of which influence what?']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What type of organism is responsible for forming nitroglycerin in certain environments?',\n",
       " 'What is the type of microorganism typically used to ferment beer and bread?',\n",
       " 'What type of organism is commonly used in production of foods such as vinegar and sauerkraut?',\n",
       " 'What type of organism is commonly used as a probiotic in fermented foods such as kimchi and sauerkraut?\\nWhat phenomenon makes atmospheric circulation patterns in the northern and southern hemispheres rotate in opposite directions?\\nChanges from a solid to a gas what phase change is an example of?\\nWhat is the most frequent radioactive decay process in nuclear reactors?',\n",
       " 'What type of microorganism produces compounds used in the production of bread and beer?',\n",
       " 'What type of microorganism is responsible for fermentation in foods like bread and beer?',\n",
       " 'What type of microorganism is responsible for fermenting foods such as sauerkraut and kimchi?',\n",
       " 'What type of fungi is commonly used to ferment foods such as soy sauce and miso?',\n",
       " 'What type of microorganism is typically used in the production of fermented foods like sauerkraut and kimchi?',\n",
       " 'What type of equipment is commonly used in breweries to ferment beer?']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_input_embeds are updated embeddings\n",
    "generate_response_with_embedding(max_new_tokens, model, prompt_input_embeds, num_samples, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, AnyStr\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from transformers import logging\n",
    "\n",
    "# logging.set_verbosity_warning()\n",
    "# temp = 0.99\n",
    "# # sample K data points from model\n",
    "# def sample_K_times(model, K):\n",
    "#     return\n",
    "\n",
    "# def embed(data : List[AnyStr]) -> torch.Tensor:\n",
    "#     max_length = 32768\n",
    "#     passage_embeddings = embedding_model.encode(data)\n",
    "#     # normalize embeddings\n",
    "#     query_embeddings = normalize(passage_embeddings)\n",
    "#     return query_embeddings\n",
    "\n",
    "# def extract_string(example):\n",
    "#   return {\"text\": example[\"question\"]}\n",
    "\n",
    "# def rbf_mmd(X, Y, sigma=1.0, chunk_size=None):\n",
    "#     gamma = 1 / (2 * sigma**2)\n",
    "#     def row_mean(v, X):\n",
    "#         dist_sqrs = torch.sum((X - v)**2, dim=1)\n",
    "#         return torch.exp(-gamma * dist_sqrs).mean()\n",
    "#     kernel_X = lambda v: row_mean(v, X)\n",
    "#     kernel_Y = lambda v: row_mean(v, Y)\n",
    "#     K_XX = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(X))\n",
    "#     K_XY = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(Y))\n",
    "#     K_YY = torch.mean(torch.vmap(kernel_Y, chunk_size=chunk_size)(Y))\n",
    "#     return K_XX + K_YY - 2 * K_XY\n",
    "\n",
    "# # generate a single random math question from the LLM, based on 3 examples. \n",
    "# # can set temperature higher to get more responses.\n",
    "# def generate_response(model, tokenizer):\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are my assistant. Please look at the examples of questions given and write a similar question with the same topic or flavour. Do not give the solution or any extra words.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"\\n\".join(dataset[\"train\"][0:5]['question'])},\n",
    "#     ]\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(model.device)\n",
    "\n",
    "#     terminators = [\n",
    "#         tokenizer.eos_token_id,\n",
    "#         tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "#     ]\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=256,\n",
    "#         pad_token_id=tokenizer.eos_token_id,\n",
    "#         eos_token_id=terminators,\n",
    "#         do_sample=True,\n",
    "#         temperature=temp,\n",
    "#         top_p=0.9,\n",
    "#     )\n",
    "#     response = outputs[0][input_ids.shape[-1]:]\n",
    "#     output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "#     return model.get_input_embeddings()(input_ids), input_ids, output\n",
    "\n",
    "# def backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, model_output, input_ids):\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False # freeze all params\n",
    "\n",
    "#     output_ids = tokenizer(model_output, return_tensors=\"pt\").input_ids.to(\"cuda:5\")\n",
    "#     # Concatenate input and output to form full sequence. This makes it easy to find the logit later.\n",
    "#     full_input = torch.cat([input_ids, output_ids], dim=-1)\n",
    "\n",
    "#     # **Extract embeddings with requires_grad=True**\n",
    "#     embedding_layer = model.get_input_embeddings()  # Embedding layer\n",
    "#     input_embeds = embedding_layer(full_input).detach().clone()  # Shape: (1, input_length + response length, hidden_size)\n",
    "\n",
    "#     # allow gradients on input embedding\n",
    "#     input_embeds.requires_grad = True  # Enable gradient tracking\n",
    "#     optimizer = torch.optim.Adam([input_embeds], lr=1.0)\n",
    "\n",
    "#     # Forward pass\n",
    "#     outputs = model(inputs_embeds=input_embeds)\n",
    "\n",
    "#     # Extract logits (this is the full sentence logit)\n",
    "#     logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "#     # Compute log-softmax to get log probabilities\n",
    "#     log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "#     # Shift output_ids for teacher forcing (we predict the next token)\n",
    "#     target_ids = full_input[:, 1:]  # Shift left for alignment\n",
    "\n",
    "#     # Gather log probabilities corresponding to the actual output tokens\n",
    "#     output_log_probs = log_probs[:, :-1, :].gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "#     # Compute total log-likelihood starting only from end of input (so only log-porbs of model response)\n",
    "#     total_log_likelihood = output_log_probs[:, input_ids.shape[-1]:].sum()\n",
    "\n",
    "#     # Backpropagate\n",
    "#     total_log_likelihood.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     return input_embeds # return the input embedding after it has been updated with gradients\n",
    "# temp = 1.0\n",
    "# def generate_response_with_embedding(max_new_tokens, model, new_embed, num_samples, input_ids):\n",
    "#     samples = []\n",
    "#     for _ in range(num_samples):\n",
    "#         generated_tokens = input_ids.clone()\n",
    "#         new_generation_input_embed = new_embed.clone()\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             outputs = model(inputs_embeds=new_generation_input_embed)\n",
    "#             logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
    "            \n",
    "#             #next_token = torch.argmax(logits, dim=-1, keepdim=True)  # Greedy decoding\n",
    "            \n",
    "#             # temperature\n",
    "#             logits = logits / temp\n",
    "#             # Convert to probabilities and sample\n",
    "#             probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "#             next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "#             # Append new token to generated sequence\n",
    "#             generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "#             # Update `input_embeds` to include new token embeddings\n",
    "#             next_token_embedding = model.get_input_embeddings()(next_token)\n",
    "#             new_generation_input_embed = torch.cat([new_generation_input_embed, next_token_embedding], dim=1)\n",
    "\n",
    "#             # Stop if EOS token is generated\n",
    "#             if next_token.item() == tokenizer.eos_token_id:\n",
    "#                 break\n",
    "#         samples.append(tokenizer.decode(generated_tokens[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "#     return samples\n",
    "\n",
    "# n=10\n",
    "# k=10\n",
    "# lr=0.001\n",
    "# all_estimated_gradients = []\n",
    "# all_similarity = []\n",
    "# for idx in range(n):\n",
    "#     print(\"getting gradient samples in iteration: \", idx)\n",
    "#     sampled_examples = [] # \n",
    "#     backpropagated_input_embeddings = [] # to be averaged later on\n",
    "#     for _ in range(k):\n",
    "#         prompt_input_embeds, input_ids, output = generate_response(model, tokenizer)\n",
    "#         input_len = len(input_ids[0])\n",
    "\n",
    "#         # note that this input embed is the FULL embedding with the input prompt and model response (because we want to get logits of the responses)\n",
    "#         # shape = [batchsize, prompt_length + response_length, embed dim]. Later on, we just need to extract the first prompt_length for inferencing.\n",
    "#         updated_input_embeds = backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, output, input_ids)\n",
    "#         sampled_examples.append(output)\n",
    "#         backpropagated_input_embeddings.append(updated_input_embeds) # updated embedding after backward pass\n",
    "\n",
    "#     logit_grad = torch.zeros_like(backpropagated_input_embeddings[0][:,:input_len,:]) # log p(x_1) + log p(x_2) + ... + p(x_k)\n",
    "#     for embedding in backpropagated_input_embeddings:\n",
    "#         grad_log_prob = embedding[:,:input_len,:] - prompt_input_embeds[:,:input_len,:] #  prompt_input_embeds is the original embedding; find the difference from updated embedding to get the gradient update.\n",
    "#         logit_grad += grad_log_prob # derivative of (log p(x_1) + log p(x_2) + ... + p(x_k))\n",
    "\n",
    "#     mmd = rbf_mmd(torch.tensor(embed(sampled_examples)), torch.tensor(embed(ground_truth))) # MMD value\n",
    "#     all_similarity.append(mmd)\n",
    "#     estimated_gradient = (1/n) * mmd * logit_grad # REINFORCE equation\n",
    "#     all_estimated_gradients.append(estimated_gradient) # each sample in REINFORCE\n",
    "\n",
    "# print(\"embedding norm before gradient update: \", prompt_input_embeds[:,:input_len,:].sum())\n",
    "# print(\"average MMD values before updated: \", np.array(all_similarity).mean())\n",
    "# # expected_gradient is average of all_estimated_gradients\n",
    "# expected_gradient = torch.stack(all_estimated_gradients).sum(dim=0)\n",
    "# new_generation_input_embed = prompt_input_embeds[:,:input_len,:] - lr * expected_gradient\n",
    "# print(\"embedding norm after gradient update: \", new_generation_input_embed[:,:input_len,:].sum())\n",
    "# print(\"========GENERATING WITH NEW INPUT EMBEDDING AFTER GRADIENT UPDATE=========\")\n",
    "\n",
    "# # Manually generate tokens step-by-step\n",
    "# max_new_tokens=50\n",
    "# num_samples = 10\n",
    "# generated_tokens = input_ids.clone()\n",
    "# new_samples = generate_response_with_embedding(max_new_tokens, model, new_generation_input_embed, num_samples, input_ids)\n",
    "# print(\"new MMD values after gradient update: \", rbf_mmd(torch.tensor(embed(new_samples)), torch.tensor(embed(ground_truth))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mix-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
