{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-11 14:21:37.537246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-11 14:21:37.537311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-11 14:21:37.538785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 14:21:37.546802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 14:21:38.298352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/chenzhil/miniconda3/envs/data-mix-new/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "\n",
    "import torch_influence\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_warn_always(False)\n",
    "\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import yaml\n",
    "import lm_eval\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training ,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from llm import get_tokenizer_and_model\n",
    "\n",
    "tokenizer, model = get_tokenizer_and_model(model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = model.to(\"cuda:5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"allenai/sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = dataset[\"test\"][100:120]['question'] # take 10 samples for ground truth for now\n",
    "train_gen = dataset[\"train\"][100:120]['question'] # take 10 samples for ground truth for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11:14:21:59,568 INFO     [SentenceTransformer.py:218] Load pretrained SentenceTransformer: Alibaba-NLP/gte-Qwen2-1.5B-instruct\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.34it/s]\n",
      "2025-02-11:14:22:06,115 INFO     [SentenceTransformer.py:357] 1 prompts are loaded, with the keys: ['query']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True, device=\"cpu\")\n",
    "embedding_model = embedding_model.to(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, AnyStr\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "temp = 0.99\n",
    "\n",
    "# embed a list of texts\n",
    "def embed(data : List[AnyStr]) -> torch.Tensor:\n",
    "    max_length = 32768\n",
    "    passage_embeddings = embedding_model.encode(data)\n",
    "    # normalize embeddings\n",
    "    #query_embeddings = normalize(passage_embeddings)\n",
    "    return passage_embeddings\n",
    "\n",
    "# mmd function\n",
    "def rbf_mmd(X, Y, sigma=1.0, chunk_size=None):\n",
    "    gamma = 1 / X.shape[1]\n",
    "    def row_mean(v, X):\n",
    "        dist_sqrs = torch.sum((X - v)**2, dim=1)\n",
    "        return torch.exp(-gamma * dist_sqrs).mean()\n",
    "    kernel_X = lambda v: row_mean(v, X)\n",
    "    kernel_Y = lambda v: row_mean(v, Y)\n",
    "    K_XX = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(X))\n",
    "    K_XY = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(Y))\n",
    "    K_YY = torch.mean(torch.vmap(kernel_Y, chunk_size=chunk_size)(Y))\n",
    "    return K_XX + K_YY - 2 * K_XY\n",
    "\n",
    "# generate a single sample from LLM, based on 3 examples. \n",
    "# can set temperature higher to get more diverse responses.\n",
    "def generate_response(model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are my assistant. Please look at the examples of questions given and write a similar question with the same topic or flavour. Do not give the solution or any extra words.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\\n\".join(dataset[\"train\"][0:5]['question'])},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "    return model.get_input_embeddings()(input_ids), input_ids, output\n",
    "\n",
    "# given a extract_string text response, find the logits from an LLM and backpropagate the gradients to the embedding values\n",
    "# input_ids are the input prompts used (so we start generating log-probs from that point)\n",
    "def backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, model_output, input_ids, prompt_embedding):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False # freeze all params\n",
    "\n",
    "    # output_ids = tokenizer(model_output, return_tensors=\"pt\").input_ids.to(\"cuda:5\")\n",
    "    # # Concatenate input and output to form full sequence. This makes it easy to find the logit later.\n",
    "    # full_input = torch.cat([input_ids, output_ids], dim=-1)\n",
    "\n",
    "    # # **Extract embeddings with requires_grad=True**\n",
    "    # embedding_layer = model.get_input_embeddings()  # Embedding layer\n",
    "    # input_embeds = embedding_layer(full_input).detach().clone()  # Shape: (1, input_length + response length, hidden_size)\n",
    "\n",
    "    # # allow gradients on input embedding\n",
    "    # input_embeds.requires_grad = True  # Enable gradient tracking\n",
    "    # optimizer = torch.optim.Adam([input_embeds], lr=1.0)\n",
    "\n",
    "    # # Forward pass\n",
    "    # outputs = model(inputs_embeds=input_embeds, disable_tqdm=True)\n",
    "\n",
    "    # # Extract logits (this is the full sentence logit)\n",
    "    # logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "    # # Compute log-softmax to get log probabilities\n",
    "    # log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # # Shift output_ids for teacher forcing (we predict the next token)\n",
    "    # target_ids = full_input[:, 1:]  # Shift left for alignment\n",
    "\n",
    "    # # Gather log probabilities corresponding to the actual output tokens\n",
    "    # output_log_probs = log_probs[:, :-1, :].gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # # Compute total negative log-likelihood starting only from end of input (so only log-probs of response)\n",
    "    # total_log_likelihood = - output_log_probs[:, input_ids.shape[-1]:].sum()\n",
    "    \n",
    "    output_ids = tokenizer(model_output, return_tensors=\"pt\").input_ids.to(\"cuda:5\")\n",
    "    # Concatenate input and output to form full sequence. This makes it easy to find the logit later.\n",
    "\n",
    "    # **Extract embeddings with requires_grad=True**\n",
    "    embedding_layer = model.get_input_embeddings()  # Embedding layer\n",
    "    output_embed = embedding_layer(output_ids).detach().clone()\n",
    "    input_embeds = prompt_embedding.detach().clone()\n",
    "    input_embeds = torch.concat([input_embeds, output_embed], dim=1)\n",
    "    \n",
    "    # allow gradients on input embedding\n",
    "    input_embeds.requires_grad = True  # Enable gradient tracking\n",
    "    optimizer = torch.optim.Adam([input_embeds], lr=1.0)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs_embeds=input_embeds, disable_tqdm=True)\n",
    "\n",
    "    # Extract logits (this is the full sentence logit)\n",
    "    logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "    # Compute log-softmax to get log probabilities\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Shift output_ids for teacher forcing (we predict the next token)\n",
    "    full_input=torch.concat([input_ids, output_ids], dim=1)\n",
    "    target_ids = full_input[:, 1:]  # Shift left for alignment\n",
    "\n",
    "    # Gather log probabilities corresponding to the actual output tokens\n",
    "    output_log_probs = log_probs[:, :-1, :].gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Compute total negative log-likelihood starting only from end of input (so only log-probs of response)\n",
    "    total_log_likelihood = output_log_probs[:, input_ids.shape[-1]:].sum()\n",
    "\n",
    "    # Backpropagate\n",
    "    total_log_likelihood.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return input_embeds # return the input embedding after it has been updated with gradients\n",
    "\n",
    "# generate an LLM response with an input embedding\n",
    "def generate_response_with_embedding(max_new_tokens, model, new_embed, num_samples, input_ids):\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        generated_tokens = input_ids.clone()\n",
    "        new_generation_input_embed = new_embed.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(inputs_embeds=new_generation_input_embed, disable_tqdm=True)\n",
    "            logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
    "            \n",
    "            #next_token = torch.argmax(logits, dim=-1, keepdim=True)  # Greedy decoding\n",
    "            \n",
    "            # temperature\n",
    "            logits = logits / temp\n",
    "            # Convert to probabilities and sample\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append new token to generated sequence\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "            # Update `input_embeds` to include new token embeddings\n",
    "            next_token_embedding = model.get_input_embeddings()(next_token)\n",
    "            new_generation_input_embed = torch.cat([new_generation_input_embed, next_token_embedding], dim=1)\n",
    "\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        samples.append(tokenizer.decode(generated_tokens[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run REINFORCE to update embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step:  0\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(30.8750, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00020962954\n",
      "gradient norm:  tensor(-8.8750, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(31., device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0002)\n",
      "training step:  1\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(31., device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00019181966\n",
      "gradient norm:  tensor(19.2500, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(30.7500, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0002)\n",
      "training step:  2\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(30.7500, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00020333528\n",
      "gradient norm:  tensor(-3.5625, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(30.7500, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0002)\n",
      "training step:  3\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(30.7500, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00021369457\n",
      "gradient norm:  tensor(-5.5938, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(30.8750, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0002)\n",
      "training step:  4\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(30.8750, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.0002029419\n",
      "gradient norm:  tensor(-9.6875, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(30.8750, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0003)\n",
      "training step:  5\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(30.8750, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.0002039671\n",
      "gradient norm:  tensor(-17.8750, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(31.1250, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0003)\n",
      "training step:  6\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(31.1250, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00021175147\n",
      "gradient norm:  tensor(-10.5625, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(31.2500, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0002)\n",
      "training step:  7\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(31.2500, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00023255349\n",
      "gradient norm:  tensor(22.3750, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(31., device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0003)\n",
      "training step:  8\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(31., device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00023503303\n",
      "gradient norm:  tensor(-24.3750, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(31.2500, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0002)\n",
      "training step:  9\n",
      "getting gradient samples in iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gradient samples in iteration:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding norm before gradient update:  tensor(31.2500, device='cuda:5', dtype=torch.bfloat16)\n",
      "average MMD values before updated:  0.00021905899\n",
      "gradient norm:  tensor(16.3750, device='cuda:5', dtype=torch.bfloat16)\n",
      "embedding norm after gradient update:  tensor(31.1250, device='cuda:5', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new MMD values after gradient update:  tensor(0.0003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# starting embedding to start gradient descent\n",
    "prompt_input_embeds, input_ids, output = generate_response(model, tokenizer)\n",
    "\n",
    "n=10 # n, same as formula\n",
    "k=20 # k, same as formula\n",
    "lr=0.01 # learning rate\n",
    "training_steps = 10 # number of GD steps.\n",
    "for step in range(training_steps):\n",
    "    print(\"training step: \", step)\n",
    "    all_estimated_gradients = []\n",
    "    all_similarity = []\n",
    "    for idx in range(n):\n",
    "        print(\"getting gradient samples in iteration: \", idx)\n",
    "        sampled_examples = [] # \n",
    "        backpropagated_input_embeddings = []\n",
    "        for _ in range(k):\n",
    "            input_len = len(input_ids[0])\n",
    "            \n",
    "            # generate a random LLM response from current embedding\n",
    "            output = generate_response_with_embedding(256, model, prompt_input_embeds, 1, input_ids)[0]\n",
    "            \n",
    "            # backpropagate to update the embedding\n",
    "            updated_input_embeds = backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, output, input_ids, prompt_input_embeds)\n",
    "            sampled_examples.append(output)\n",
    "            backpropagated_input_embeddings.append(updated_input_embeds)\n",
    "\n",
    "        logit_grad = torch.zeros_like(backpropagated_input_embeddings[0][:,:input_len,:]) # log p(x_1) + log p(x_2) + ... + p(x_k)\n",
    "        for embedding in backpropagated_input_embeddings:\n",
    "            \n",
    "            # calculate the gradient based on updated embedding i.e., derivative of log P(X)\n",
    "            grad_log_prob = embedding[:,:input_len,:] - prompt_input_embeds[:,:input_len,:]\n",
    "            \n",
    "            # log P(X_1) + ... + P(X_k)\n",
    "            logit_grad += grad_log_prob\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # compute MMD of current k samples\n",
    "            mmd = rbf_mmd(torch.tensor(embed(sampled_examples)), torch.tensor(embed(ground_truth))) # MMD value\n",
    "\n",
    "        all_similarity.append(mmd)\n",
    "        estimated_gradient_sample = mmd * logit_grad # REINFORCE equation to get one sample of logit gradient\n",
    "        all_estimated_gradients.append(estimated_gradient_sample)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(\"embedding norm before gradient update: \", prompt_input_embeds[:,:input_len,:].sum())\n",
    "        print(\"average MMD values before updated: \", np.array(all_similarity).mean())\n",
    "        expected_gradient = torch.stack(all_estimated_gradients).sum(dim=0) * (1/n) # expected gradient\n",
    "        print(\"gradient norm: \", expected_gradient.sum())\n",
    "        prompt_input_embeds = prompt_input_embeds - lr * expected_gradient # update embedding with gradient\n",
    "        print(\"embedding norm after gradient update: \", prompt_input_embeds[:,:input_len,:].sum())\n",
    "\n",
    "        # check MMD for newly generated samples\n",
    "        max_new_tokens=256\n",
    "        num_samples = 10\n",
    "        generated_tokens = input_ids.clone()\n",
    "        new_samples = generate_response_with_embedding(max_new_tokens, model, prompt_input_embeds, num_samples, input_ids)\n",
    "        print(\"new MMD values after gradient update: \", rbf_mmd(torch.tensor(embed(new_samples)), torch.tensor(embed(ground_truth))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where is the spinal trigeminal nucleus located?',\n",
       " 'The lithosphere is divided into a dozen major and several minor what?',\n",
       " 'During the first year after birth, what is a baby called?',\n",
       " 'What are used to indicate the number of atoms of an element that are in the compound?',\n",
       " 'Area, volume, and speed are all examples of what type of units?',\n",
       " 'Anything moving has what type of energy?',\n",
       " 'A skydiver will reach what when the air drag equals their weight?',\n",
       " 'What organs are considered the female gonads?',\n",
       " 'What is the adaptation that certain animals use to become less visible to predators and prey?',\n",
       " 'What is another term for blood clotting?',\n",
       " 'What do you call the study of how organisms interact with their environment and each other?',\n",
       " 'Childbirth usually starts when which sac breaks?',\n",
       " 'What phenomenon, which is most important in small populations, occurs because the alleles in an offspring generation are a random sample of the alleles in the parent generation?',\n",
       " 'What is the term for when an embryo fixes itself to the side of the uterus?',\n",
       " 'When exposed to ultraviolet, some substances, such as minerals, glow in characteristic visible wavelengths, a process called this?',\n",
       " 'What disease occurs when the cell cycle is no longer regulated?',\n",
       " 'If a lump of clay is dropped into water, what will occur?',\n",
       " 'Nerve cells that sense touch are found mainly where?',\n",
       " 'What do concentric circles on a topographic map indicate?',\n",
       " 'Arthropods today have two unusual hox genes, both of which influence what?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the primary difference in composition between granite and basalt rocks?',\n",
       " \"What is the primary medium used for heat transfer in the earth's core?\",\n",
       " 'What is the primary component of the stars in our galaxy, including our sun, and is also found in our body in the form of calcium in bones and teeth?',\n",
       " 'What type of sounds are produced by the wind blowing over sand ridges and dunes?',\n",
       " 'What type of insect is known for its distinctive \"waggle dance\" used for communication?',\n",
       " 'Gases',\n",
       " 'What is the process by which the moon appears to move along a path in the sky that changes shape as it orbits around the earth?',\n",
       " \"What is the primary component of a solar flare's energy spectrum?\",\n",
       " \"What type of rock is typically formed from the cooling and solidification of magma deep within the Earth's crust?\",\n",
       " 'What is a prominent characteristic of fiery meteor showers?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_input_embeds are updated embeddings\n",
    "generate_response_with_embedding(max_new_tokens, model, prompt_input_embeds, num_samples, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, AnyStr\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from transformers import logging\n",
    "\n",
    "# logging.set_verbosity_warning()\n",
    "# temp = 0.99\n",
    "\n",
    "# # embed a list of texts\n",
    "# def embed(data : List[AnyStr]) -> torch.Tensor:\n",
    "#     max_length = 32768\n",
    "#     passage_embeddings = embedding_model.encode(data)\n",
    "#     # normalize embeddings\n",
    "#     query_embeddings = normalize(passage_embeddings)\n",
    "#     return query_embeddings\n",
    "\n",
    "# # mmd function\n",
    "# def rbf_mmd(X, Y, sigma=1.0, chunk_size=None):\n",
    "#     gamma = 1 / X.shape[1]\n",
    "#     def row_mean(v, X):\n",
    "#         dist_sqrs = torch.sum((X - v)**2, dim=1)\n",
    "#         return torch.exp(-gamma * dist_sqrs).mean()\n",
    "#     kernel_X = lambda v: row_mean(v, X)\n",
    "#     kernel_Y = lambda v: row_mean(v, Y)\n",
    "#     K_XX = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(X))\n",
    "#     K_XY = torch.mean(torch.vmap(kernel_X, chunk_size=chunk_size)(Y))\n",
    "#     K_YY = torch.mean(torch.vmap(kernel_Y, chunk_size=chunk_size)(Y))\n",
    "#     return K_XX + K_YY - 2 * K_XY\n",
    "\n",
    "# # generate a single sample from LLM, based on 3 examples. \n",
    "# # can set temperature higher to get more diverse responses.\n",
    "# def generate_response(model, tokenizer):\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are my assistant. Please look at the examples of questions given and write a similar question with the same topic or flavour. Do not give the solution or any extra words.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"\\n\".join(dataset[\"train\"][0:5]['question'])},\n",
    "#     ]\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(model.device)\n",
    "\n",
    "#     terminators = [\n",
    "#         tokenizer.eos_token_id,\n",
    "#         tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "#     ]\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=256,\n",
    "#         pad_token_id=tokenizer.eos_token_id,\n",
    "#         eos_token_id=terminators,\n",
    "#         do_sample=True,\n",
    "#         temperature=temp,\n",
    "#         top_p=1.0,\n",
    "#     )\n",
    "#     response = outputs[0][input_ids.shape[-1]:]\n",
    "#     output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "#     return model.get_input_embeddings()(input_ids), input_ids, output\n",
    "\n",
    "# # given a extract_string text response, find the logits from an LLM and backpropagate the gradients to the embedding values\n",
    "# # input_ids are the input prompts used (so we start generating log-probs from that point)\n",
    "# def backpropagate_gradients_to_embedding_based_on_logits(model, tokenizer, model_output, input_ids, prompt_embedding):\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False # freeze all params\n",
    "\n",
    "#     # output_ids = tokenizer(model_output, return_tensors=\"pt\").input_ids.to(\"cuda:5\")\n",
    "#     # # Concatenate input and output to form full sequence. This makes it easy to find the logit later.\n",
    "#     # full_input = torch.cat([input_ids, output_ids], dim=-1)\n",
    "\n",
    "#     # # **Extract embeddings with requires_grad=True**\n",
    "#     # embedding_layer = model.get_input_embeddings()  # Embedding layer\n",
    "#     # input_embeds = embedding_layer(full_input).detach().clone()  # Shape: (1, input_length + response length, hidden_size)\n",
    "\n",
    "#     # # allow gradients on input embedding\n",
    "#     # input_embeds.requires_grad = True  # Enable gradient tracking\n",
    "#     # optimizer = torch.optim.Adam([input_embeds], lr=1.0)\n",
    "\n",
    "#     # # Forward pass\n",
    "#     # outputs = model(inputs_embeds=input_embeds, disable_tqdm=True)\n",
    "\n",
    "#     # # Extract logits (this is the full sentence logit)\n",
    "#     # logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "#     # # Compute log-softmax to get log probabilities\n",
    "#     # log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "#     # # Shift output_ids for teacher forcing (we predict the next token)\n",
    "#     # target_ids = full_input[:, 1:]  # Shift left for alignment\n",
    "\n",
    "#     # # Gather log probabilities corresponding to the actual output tokens\n",
    "#     # output_log_probs = log_probs[:, :-1, :].gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "#     # # Compute total negative log-likelihood starting only from end of input (so only log-probs of response)\n",
    "#     # total_log_likelihood = - output_log_probs[:, input_ids.shape[-1]:].sum()\n",
    "    \n",
    "#     output_ids = tokenizer(model_output, return_tensors=\"pt\").input_ids.to(\"cuda:5\")\n",
    "#     # Concatenate input and output to form full sequence. This makes it easy to find the logit later.\n",
    "\n",
    "#     # **Extract embeddings with requires_grad=True**\n",
    "#     embedding_layer = model.get_input_embeddings()  # Embedding layer\n",
    "#     output_embed = embedding_layer(output_ids).detach().clone()\n",
    "#     input_embeds = prompt_embedding.detach().clone()\n",
    "#     input_embeds = torch.concat([input_embeds, output_embed], dim=1)\n",
    "    \n",
    "#     # allow gradients on input embedding\n",
    "#     input_embeds.requires_grad = True  # Enable gradient tracking\n",
    "#     optimizer = torch.optim.Adam([input_embeds], lr=1.0)\n",
    "\n",
    "#     # Forward pass\n",
    "#     outputs = model(inputs_embeds=input_embeds, disable_tqdm=True)\n",
    "\n",
    "#     # Extract logits (this is the full sentence logit)\n",
    "#     logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "#     # Compute log-softmax to get log probabilities\n",
    "#     log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "#     # Shift output_ids for teacher forcing (we predict the next token)\n",
    "#     full_input=torch.concat([input_ids, output_ids], dim=1)\n",
    "#     target_ids = full_input[:, 1:]  # Shift left for alignment\n",
    "\n",
    "#     # Gather log probabilities corresponding to the actual output tokens\n",
    "#     output_log_probs = log_probs[:, :-1, :].gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "#     # Compute total negative log-likelihood starting only from end of input (so only log-probs of response)\n",
    "#     total_log_likelihood = - output_log_probs[:, input_ids.shape[-1]:].sum()\n",
    "\n",
    "#     # Backpropagate\n",
    "#     total_log_likelihood.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     return input_embeds # return the input embedding after it has been updated with gradients\n",
    "\n",
    "# # generate an LLM response with an input embedding\n",
    "# def generate_response_with_embedding(max_new_tokens, model, new_embed, num_samples, input_ids):\n",
    "#     samples = []\n",
    "#     for _ in range(num_samples):\n",
    "#         generated_tokens = input_ids.clone()\n",
    "#         new_generation_input_embed = new_embed.clone()\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             outputs = model(inputs_embeds=new_generation_input_embed, disable_tqdm=True)\n",
    "#             logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
    "            \n",
    "#             #next_token = torch.argmax(logits, dim=-1, keepdim=True)  # Greedy decoding\n",
    "            \n",
    "#             # temperature\n",
    "#             logits = logits / temp\n",
    "#             # Convert to probabilities and sample\n",
    "#             probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "#             next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "#             # Append new token to generated sequence\n",
    "#             generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)\n",
    "\n",
    "#             # Update `input_embeds` to include new token embeddings\n",
    "#             next_token_embedding = model.get_input_embeddings()(next_token)\n",
    "#             new_generation_input_embed = torch.cat([new_generation_input_embed, next_token_embedding], dim=1)\n",
    "\n",
    "#             # Stop if EOS token is generated\n",
    "#             if next_token.item() == tokenizer.eos_token_id:\n",
    "#                 break\n",
    "#         samples.append(tokenizer.decode(generated_tokens[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "#     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mix-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
